{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c07ae79-c69e-4351-bfee-afc033e7a1bc",
   "metadata": {},
   "source": [
    "# Construir um Chatbot\n",
    "\n",
    "Vamos apresentar um exemplo de como projetar e implementar um chatbot alimentado por um LLM. Este chatbot será capaz de ter uma conversa e lembrar interações anteriores. Observe que este chatbot que estamos construindo usará apenas o modelo de linguagem para manter uma conversa. Existem vários outros conceitos relacionados que você pode estar procurando:\n",
    "\n",
    "- Conversacional RAG: Habilitar uma experiência de chatbot sobre uma fonte externa de dados\n",
    "- Agentes: Construir um chatbot que possa realizar ações\n",
    "Este tutorial cobrirá o básico, que será útil para esses dois tópicos mais avançados, mas sinta-se à vontade para ir diretamente a eles, se preferir.\n",
    "\n",
    "Aqui estão alguns dos componentes de alto nível com os quais trabalharemos:\n",
    "\n",
    "- Chat Models. A interface do chatbot é baseada em mensagens em vez de texto bruto, sendo mais adequada para Modelos de Chat do que para LLMs de texto.\n",
    "- Prompt Templates, que simplificam o processo de montagem de prompts que combinam mensagens padrão, entrada do usuário, histórico do chat e (opcionalmente) contexto adicional obtido.\n",
    "- Chat History, que permite ao chatbot \"lembrar\" interações passadas e levá-las em consideração ao responder a perguntas de acompanhamento.\n",
    "- Depuração e rastreamento de sua aplicação usando LangSmith\n",
    "\n",
    "Vamos cobrir como integrar os componentes acima para criar um chatbot conversacional poderoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7348e4-89c2-4b7b-affa-34255d0834c2",
   "metadata": {},
   "source": [
    "### Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6fcb44-a4f5-4b4c-9a54-03885a37c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (0.2.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (0.1.77)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22908dd9-9def-40a5-8382-ef684db51595",
   "metadata": {},
   "source": [
    "### LangSmith\n",
    "\n",
    "Muitas das aplicações que você construir com LangChain conterão múltiplas etapas com várias invocações de chamadas de LLM. À medida que essas aplicações se tornam mais complexas, torna-se crucial a capacidade de inspecionar exatamente o que está acontecendo dentro da sua cadeia ou agente. A melhor maneira de fazer isso é com LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84c3f18-f190-498e-bddd-b4bcb22ff34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50f2cd-4834-48f5-8d60-1ef40d2a7ae6",
   "metadata": {},
   "source": [
    "### Escolha do modelo\n",
    "\n",
    "Primeiramente, vamos aprender como usar um modelo de linguagem por si só. LangChain suporta vários modelos de linguagem diferentes que você pode usar de forma intercambiável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "800fc0c1-09bf-4672-872b-f8f4c49ec6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-mistralai in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (0.1.8)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-mistralai) (0.27.0)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-mistralai) (0.4.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-mistralai) (0.2.5)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-mistralai) (0.19.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.2.0->langchain-mistralai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.2.0->langchain-mistralai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.2.0->langchain-mistralai) (0.1.77)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.2.0->langchain-mistralai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.2.0->langchain-mistralai) (2.7.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3,>=0.2.0->langchain-mistralai) (8.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from tokenizers<1,>=0.15.1->langchain-mistralai) (0.23.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2024.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.0->langchain-mistralai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2.0->langchain-mistralai) (3.10.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.0->langchain-mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.0->langchain-mistralai) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54e956ac-1441-402a-b368-9dbf59f6884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebca430-5ff6-41a9-b179-ab56ac1bc35a",
   "metadata": {},
   "source": [
    "Primeiro, vamos usar o modelo diretamente. Os ChatModels são instâncias de \"Runnables\" do LangChain, o que significa que eles expõem uma interface padrão para interagir com eles. Para simplesmente chamar o modelo, podemos passar uma lista de mensagens para o método .invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c62e7deb-efef-434b-a48f-60a7c843c0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Bob! It's nice to meet you. How can I assist you today?\", response_metadata={'token_usage': {'prompt_tokens': 9, 'total_tokens': 27, 'completion_tokens': 18}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-42dfd1dd-4e92-4ca5-87bd-63df4afeb39f-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab6c01-3c9b-4b27-a732-b751a3c8b398",
   "metadata": {},
   "source": [
    "O modelo por si só não tem nenhum conceito de estado. Por exemplo, se você fizer uma pergunta de acompanhamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f7d825-5090-4e40-947e-37b42f24c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Infelizmente, eu não tenho acesso a informações pessoais sobre você, como seu nome, a menos que você me informe. Se você quiser me dizer o seu nome, ficarei feliz em me lembrar durante nossa conversa.', response_metadata={'token_usage': {'prompt_tokens': 9, 'total_tokens': 76, 'completion_tokens': 67}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-c8db9be9-deed-4dfb-88ba-daff8bbe86a8-0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"Qual o meu nome?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45092444-1000-4996-98e8-caa773a2727a",
   "metadata": {},
   "source": [
    "Podemos ver que ele não considera o turno anterior da conversa como contexto e não consegue responder à pergunta. Isso resulta em uma experiência de chatbot terrível!\n",
    "\n",
    "Para contornar isso, precisamos passar todo o histórico da conversa para o modelo. Vamos ver o que acontece quando fazemos isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4139d31b-28d2-438d-80f1-322d976ca8eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Seu nome é João. Como posso ajudar você hoje, João?\\n\\nEstou aqui para responder suas perguntas e fornecer informações úteis sobre uma variedade de tópicos. Se tiver alguma dúvida ou precisar de ajuda com algo, sinta-se à vontade para perguntar!\\n\\nPor exemplo, posso ajudar a responder perguntas sobre notícias atuais, informações gerais, esportes, entretenimento, tecnologia, educação, saúde e muito mais. Se tiver alguma dúvida ou precisar de ajuda com algo, sinta-se à vontade para perguntar!\\n\\nEstou aqui para ajudar você de todas as maneiras possíveis. Então, sinta-se à vontade para fazer qualquer pergunta que você tenha em mente, e farei o meu melhor para responder com precisão e clareza.', response_metadata={'token_usage': {'prompt_tokens': 35, 'total_tokens': 284, 'completion_tokens': 249}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-2c5d5388-7140-4d8b-936b-bc5ff4dee600-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Olá! eu sou o João\"),\n",
    "        AIMessage(content=\"Oi João! Como você está se sentindo hoje?\"),\n",
    "        HumanMessage(content=\"Qual o meu nome?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023dc60-72ea-4d4c-ac96-a2c3b69d1b93",
   "metadata": {},
   "source": [
    "E agora podemos ver que obtemos uma boa resposta!\n",
    "\n",
    "Essa é a ideia básica que sustenta a capacidade de um chatbot de interagir de forma conversacional. Então, como podemos implementar isso da melhor maneira?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18fb33-5930-459a-9c32-c05573eafde3",
   "metadata": {},
   "source": [
    "### Historico de mensagens\n",
    "\n",
    "Podemos usar uma classe de Histórico de Mensagens para envolver nosso modelo e torná-lo com estado. Isso manterá o controle das entradas e saídas do modelo e as armazenará em algum banco de dados. Interações futuras então carregarão essas mensagens e as passarão para a cadeia como parte da entrada. Vamos ver como fazer isso!\n",
    "\n",
    "Primeiro, certifique-se de instalar o langchain-community, pois vamos usar uma integração lá para armazenar o histórico de mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a3a62f4-d040-4a61-b7aa-8e0dddf83ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (0.2.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (0.1.77)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain_community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\joao.calmeida\\documents\\langchain\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe74d3a-561d-4c7b-89a7-1361c2e34671",
   "metadata": {},
   "source": [
    "Depois disso, podemos importar as classes relevantes e configurar nossa cadeia que envolve o modelo e adiciona esse histórico de mensagens. Uma parte crucial aqui é a função que passamos como get_session_history. Espera-se que essa função receba um session_id e retorne um objeto de Histórico de Mensagens. Esse session_id é usado para distinguir entre conversas separadas e deve ser passado como parte da configuração ao chamar a nova cadeia (mostraremos como fazer isso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da333733-e5ff-4a1f-9217-6da76a3f363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2862a80a-b5d8-4c4a-8336-bec09e12d226",
   "metadata": {},
   "source": [
    "Agora precisamos criar uma 'config' que passamos para o runnable a cada vez. Essa config contém informações que não fazem parte diretamente da entrada, mas que ainda são úteis. Neste caso, queremos incluir um session_id. Isso deve ficar assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e26a994a-85e7-494c-9664-ec3caf40c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 79baf7e5-1f22-4925-8f7d-0a857387e082 not found for run 8704ba39-ce1a-4426-958b-061ec386ea5d. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Olá Paulo! É um prazer te conhecer. Então, você tem 25 anos e está pronto para explorar o mundo da programação? Vamos começar essa jornada juntos! Se você tiver alguma dúvida ou precisar de ajuda, estou aqui para te ajudar.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Olá! Eu sou o Paulo e tenho 25 anos\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09a5523-b5b3-4bff-bbd8-e17361c27d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 056b1f03-6457-4e43-a7be-55bbc61920b9 not found for run 83f4d3f7-da40-4828-8caf-44f97afb2aae. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Baseado na nossa última conversa, você disse que seu nome é Paulo. Se isso estiver correto, posso confirmar que seu nome é Paulo. Caso contrário, por favor, me informe o seu nome correto para que eu possa me referir a você corretamente.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Qual é o meu nome?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c03152-60ca-41b3-8e15-27e720f28d75",
   "metadata": {},
   "source": [
    "Ótimo! Nosso chatbot agora lembra informações sobre nós. Se alterarmos a configuração para referenciar um session_id diferente, podemos ver que ele inicia a conversa do zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b04d4e07-d7b7-4f13-983c-6b1acd3a32bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run b9cd9093-ea0b-4c60-8558-531dd27b99c8 not found for run 508b92bc-1017-4b21-89c6-9aa9f9712644. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Infelizmente, eu não tenho acesso a informações pessoais sobre você, como seu nome, a menos que você mesmo me informe. Se quiser, você pode me dizer o seu nome e eu farei o possível para lembrar durante nossa conversa.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Qual é o meu nome?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e3d09-222d-4a37-aa3f-f7fe08164219",
   "metadata": {},
   "source": [
    "No entanto, sempre podemos voltar à conversa original (já que estamos persistindo isso em um banco de dados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c632136-b59e-45c6-98c9-55bc73742362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run dc6f1f93-d904-4ffa-afcc-312d7f692cb4 not found for run 01444d42-8715-4904-80c2-4d7897b77a47. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Na nossa última conversa, você mencionou que tem 25 anos. Se isso ainda estiver correto, posso confirmar que você tem 25 anos. Caso contrário, por favor, me informe sua idade atualizada para que eu possa ter essa informação correta.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Eu tenho quantos anos?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec1d95-a63e-467c-8115-bbf60ad54564",
   "metadata": {},
   "source": [
    "Assim é como podemos suportar um chatbot tendo conversas com muitos usuários!\n",
    "\n",
    "Por enquanto, tudo o que fizemos foi adicionar uma camada simples de persistência em torno do modelo. Podemos começar a torná-lo mais complicado e personalizado adicionando um modelo de prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0de8b9-ad17-486a-8324-19d2a7f57800",
   "metadata": {},
   "source": [
    "#### Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c75a4-0545-490a-a834-ea175a93eabc",
   "metadata": {},
   "source": [
    "Os prompts templates ajudam a transformar informações brutas do usuário em um formato com o qual o LLM pode trabalhar. Neste caso, a entrada bruta do usuário é apenas uma mensagem, que estamos passando para o LLM. Vamos complicar um pouco isso agora. Primeiro, vamos adicionar uma mensagem de sistema com algumas instruções personalizadas (ainda usando mensagens como entrada). Em seguida, vamos adicionar mais entradas além das mensagens.\n",
    "\n",
    "Primeiro, vamos incluir uma mensagem de sistema. Para fazer isso, criaremos um ChatPromptTemplate. Vamos usar MessagesPlaceholder para passar todas as mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03a00d79-d1a4-491b-abf3-6c702e13b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Você é um assistente prestativo. Responda a todas as perguntas da melhor maneira possível.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c93578-244b-44ad-bebc-a92b5c36d0b8",
   "metadata": {},
   "source": [
    "Observe que isso muda um pouco o tipo de entrada - em vez de passar uma lista de mensagens, agora estamos passando um dicionário com uma chave messages que contém uma lista de mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e136d490-e060-4063-a123-ae498a6465c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Olá Pedro! Estou aqui para ajudá-lo. Sinta-se à vontade para fazer suas perguntas e farei o meu melhor para responder de forma precisa e útil.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"Olá, sou Pedro\")]})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a76c33-2370-4d82-ae69-5c858c3dcac0",
   "metadata": {},
   "source": [
    "Agora podemos encapsular isso no mesmo objeto Messages History de antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d7df02b-2990-4a63-ac91-40571acd9875",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ce3e7e6-0296-4c72-8c9a-f4c0b6ee73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d50e82ec-3dce-435e-b62d-9c710a4477ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 4d2ef429-1534-4bdd-84dc-3767252ebd31 not found for run 8fc6b716-dbbe-4ba3-9fa5-8eb860731be9. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Obrigado por me dizer o seu nome, Carlos! É um prazer fazer sua conhecimento. Estou aqui para responder às suas perguntas da melhor maneira possível, então por favor, sinta-se à vontade para perguntar o que você quiser.\\n\\nThank you for letting me know your name, Carlos! It's nice to meet you. I'm here to answer your questions to the best of my ability, so please feel free to ask me anything you want.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Olá, meu nome é Carlos\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fad60322-45fe-4266-a83a-a94d3e345f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 388b5081-8627-4f7b-999e-270eb85b1e00 not found for run c4f358fe-3935-486f-a91d-7bfcc7d83e8e. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sim, Carlos, eu farei meu melhor para responder às suas perguntas da melhor maneira possível.\\n\\nVocê me disse que o seu nome é Carlos. É isso mesmo, certo? Se eu estiver enganado, por favor me corrija.\\n\\nYes, Carlos, I will do my best to answer your questions to the best of my ability.\\n\\nYou told me that your name is Carlos. Is that correct? If I am mistaken, please let me know.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Qual o meu nome?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a774a-09d1-4059-a106-a93e4c6767a0",
   "metadata": {},
   "source": [
    "Incrível! Vamos agora tornar nosso prompt um pouco mais complicado. Vamos supor que o modelo de prompt agora se pareça com isto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aa7150d-c363-489e-a277-c2c6ae382d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Você é um assistente prestativo. Responda a todas as perguntas da melhor maneira possível em {idioma}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98527b93-f402-4ebf-bc94-8a22f300a042",
   "metadata": {},
   "source": [
    "Observe que adicionamos uma nova entrada de idioma ao prompt. Agora podemos invocar a cadeia e passar um idioma de nossa escolha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72b7e342-cfde-4c8a-a3c7-d869fc73b244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿Cómo estás? Soy un asistente diseñado para ayudarte. ¿En qué puedo ayudarte hoy?\\n\\nRecuerda que aunque prefiero hablar en español, también puedo entender y responder en otros idiomas si es necesario. ¿Cómo puedo asistirte hoy?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"idioma\": \"Spanish\"}\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af77021-165c-4325-a27c-0cece1a0d28a",
   "metadata": {},
   "source": [
    "Vamos agora encapsular esta cadeia mais complicada em uma classe Message History. Desta vez, porque há múltiplas chaves na entrada, precisamos especificar a chave correta a ser usada para salvar o histórico do chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a4d7265-3f39-4f24-bdc6-eff36c6ca127",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c9c9f6b-3c7b-41d5-8a03-fad1a1c281c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2efbeff8-821e-40fe-b4d1-f260f46362e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 129679f7-5243-47e2-8768-75c519e6b7f5 not found for run 247f4145-6f0a-4155-9eb8-0fee8faf1f96. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hola, soy Todd, un asistente dispuesto a ayudarte. Voy a responder a todas tus preguntas de la mejor manera posible en español. ¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"idioma\": \"Spanish\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3ddd63d-3b84-45fd-9776-51c285121053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 02560950-6134-4178-96a4-2514846bf22b not found for run e14fb415-4ba5-4388-baf5-1df270a7418f. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Basándome en la información que tengo, tu nombre es Todd. Sin embargo, si me estás pidiendo que adivine tu nombre, lamento decirte que no puedo hacerlo porque no tengo la capacidad de adivinar. Si me proporcionas tu nombre, estaré encantado de dirigirme a ti por él.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"idioma\": \"Spanish\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8b84e-e6a5-4058-9b40-8bb3fa60f363",
   "metadata": {},
   "source": [
    "### Gerenciar o histórico da conversa\n",
    "\n",
    "Um conceito importante ao construir chatbots é como gerenciar o histórico da conversa. Se não for gerenciado adequadamente, a lista de mensagens crescerá ilimitadamente e poderá exceder a janela de contexto do LLM (Modelo de Linguagem Grande). Portanto, é importante adicionar um passo que limite o tamanho das mensagens que você está passando.\n",
    "\n",
    "É crucial fazer isso ANTES do modelo de prompt, mas DEPOIS de carregar as mensagens anteriores do histórico de mensagens.\n",
    "\n",
    "Podemos fazer isso adicionando um passo simples antes do prompt que modifica a chave das mensagens adequadamente e, em seguida, encapsular essa nova cadeia na classe Message History. Primeiro, vamos definir uma função que modificará as mensagens passadas. Vamos fazer com que ela selecione as k mensagens mais recentes. Em seguida, podemos criar uma nova cadeia adicionando isso no início."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7919b66b-4904-4f18-91f3-d646578ba4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def filter_messages(messages, k=10):\n",
    "    return messages[-k:]\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=lambda x: filter_messages(x[\"messages\"]))\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ec73f-20d1-4ca5-8f87-c18285298c0c",
   "metadata": {},
   "source": [
    "Vamos tentar agora! Se criarmos uma lista de mensagens com mais de 10 mensagens, podemos ver que ele não lembrará mais das informações nas mensagens mais antigas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "679639df-ade2-4db3-9c73-1bd69488d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "102db303-2567-4d9d-bf44-a7a4a7f49b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpa, você ainda não me disse o seu nome. Como posso te chamar?'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"idioma\": \"Portuguese\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d056316-773d-4044-bf6b-45d08b2de43d",
   "metadata": {},
   "source": [
    "Mas se perguntarmos sobre informações que estão nas últimas dez mensagens, ele ainda se lembrará delas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "361f33a9-502a-46db-bda3-66bd8f27ea42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Desculpe, não sei qual é o seu sabor favorito de sorvete. É vanila, chocolate, morango ou outro sabor?'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my fav ice cream\")],\n",
    "        \"idioma\": \"Portuguese\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea794c5f-3171-4b09-8f5a-9861f7a2fe0c",
   "metadata": {},
   "source": [
    "Vamos agora encapsular isso no Message History."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a81f43e-ae41-4a77-9e7d-68052392456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "403c4cc6-d63a-4e77-866c-83a951e7fff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 676c3789-6472-4ee2-aa6f-053bb6ed43f3 not found for run eaa94d2b-fece-4d23-9dba-c2995d1ab7cc. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Desculpe, você ainda não me disse o seu nome. Como posso te chamar?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"idioma\": \"Portuguese\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c80c1-db8e-422e-822d-d7778c11e724",
   "metadata": {},
   "source": [
    "Agora há duas novas mensagens no histórico de bate-papo. Isso significa que ainda mais informações que estavam acessíveis em nosso histórico de conversas não estão mais disponíveis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca01bdaf-a5f3-4798-b72d-7fd523e7c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 1cc70a05-1aa7-4c0f-973a-4845a82b78ee not found for run 37f816bd-26fb-4739-bacf-2eba53004c34. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Desculpe, como eu não conheço você pessoalmente, eu não sei qual é o seu sorvete favorito. Eu posso ajudar a encontrar algumas opções populares de sorvete, se você gostaria.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"whats my favorite ice cream?\")],\n",
    "        \"idioma\": \"Portuguese\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e305141d-a46d-4a6f-bd8b-a444b17833e2",
   "metadata": {},
   "source": [
    "### streaming\n",
    "\n",
    "Agora temos uma função chatbot. No entanto, uma consideração UX realmente importante para aplicativos de chatbot é o streaming. LLMs às vezes podem demorar para responder, então, para melhorar a experiência do usuário, muitos aplicativos fazem o streaming de volta de cada token conforme ele é gerado. Isso permite que o usuário veja o progresso.\n",
    "\n",
    "Na verdade, é muito fácil fazer isso!\n",
    "\n",
    "Todas as cadeias (chains) expõem um método .stream, e aquelas que usam histórico de mensagens não são diferentes. Podemos simplesmente usar esse método para obter uma resposta em streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c41c7387-81a8-4d2d-bcc0-d5166d5c6045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 373494a4-3c10-41b6-aac7-70f7b11f5c8e not found for run bf614122-86f9-46aa-9070-3889b30de2fa. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hello| Todd|!| Sure|,| here|'|s| a| joke| for| you|:|\n",
      "\n",
      "P|or| que| o| gol|fin|ho| nad|ava| em| volta| do| t|ub|ar|ão|?|\n",
      "\n",
      "P|or|que| qu|eria| faz|er| um| \"|t|ub|ar|ol|ho|\"|!|\n",
      "\n",
      "(|Why| was| the| dol|ph|in| swimming| around| the| sh|ark|?| Because| it| wanted| to| make| a| \"|sh|ark|-|ole|\"|!)||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")],\n",
    "        \"idioma\": \"Portuguese\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44709331-9cf5-42da-9c0b-1e979915d4be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
